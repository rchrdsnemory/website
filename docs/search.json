[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Learning Ressources\n\n\n\nR stats\n\n\n\nAn overview of a couple of freely available online ressources to learn the basics of R and git\n\n\n\n\n\nNov 17, 2024\n\n\nJan Pfänder\n\n\n\n\n\n\n\n\n\n\n\n\nTracked changes for revisions\n\n\n\nrevisions\n\ntracked changes\n\npublishing\n\n\n\nA quick workflow on how to track changes during revisions\n\n\n\n\n\nSep 11, 2024\n\n\nJan Pfänder\n\n\n\n\n\n\n\n\n\n\n\n\nMeta analysis for absolute beginners\n\n\n\nmeta-analysis\n\nstats\n\nsystematic review\n\n\n\nA hands-on introduction to the tip of the pyramid of evidence\n\n\n\n\n\nJun 25, 2024\n\n\nJan Pfänder\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "teaching/research_design/index.html",
    "href": "teaching/research_design/index.html",
    "title": "Epistémologie et Esprit Critique (or, Introduction to research design)",
    "section": "",
    "text": "go to course website\nIn science, but also in politics, in court, in airport-bestsellers, or at your lunch table, claims are made based on data and science. In this course, you will learn to think critically about data, and whether it actually supports the claim it is meant to.\nThis class is hands-on, meaning that you will learn how to design a study, organize a project, analyze data, present results and read scientific literature. To understand basic concepts in statistics, you will learn how to simulate data R.\nThe class is structured into four main blocks.\n1. Data Analysis - You will develop the competences to understand and analyze data\n2. Statistics - You will learn basic statistical concepts.\n3. Causal Inference - You will get an understanding of how different research designs do or do not permit causal claims.\n4. Science - You will get an idea of how science works, including how evidence accumulates and how scientific publishing works"
  },
  {
    "objectID": "blog/2024-06_meta-analysis/index.html",
    "href": "blog/2024-06_meta-analysis/index.html",
    "title": "Meta analysis for absolute beginners",
    "section": "",
    "text": "This is a beginners’ introduction to meta-analysis, simply because I myself am quite a beginner. So this is all I can do I’m afraid. My colleague Sacha Altay and I tried ourselves on our first ever meta-analysis very recently. Our main research question was whether people can tell true news from false news. Quite a hot topic these days, it seems.\nThroughout this post, I’ll use our study as an example to illustrate crucial steps of a meta-analysis. These include (i) the systematic literature search, (ii) standardized effect sizes, and (iii) meta-analytic models.\nThe idea is for this to be a hands-on introduction, so try to follow the instructions and perform parts of these steps yourself. I hope this will help you understood how a (very basic kind of) meta-analysis works, and give you some idea on how to perform one yourself."
  },
  {
    "objectID": "blog/2024-06_meta-analysis/index.html#why-care-about-meta-analysis",
    "href": "blog/2024-06_meta-analysis/index.html#why-care-about-meta-analysis",
    "title": "Meta analysis for absolute beginners",
    "section": "Why care about meta-analysis ?",
    "text": "Why care about meta-analysis ?\nYou have most likely heard about the “Pyramid of evidence”. There are different versions of the pyramid, but they have one thing in common: at the very top, there are meta-analyses.\n\n\n\nA typical textbook Pyramid of Evidence\n\n\nThe basic idea of meta-analysis is easy: the more data the better. Results of single studies can be affected by a ton of factors, e.g. the country they were run in, the experimental setup the researchers, the sample they recruit and so on. But if we average across many studies on the same thing, we get a more robust, generalizable result. And comparing many studies, we might even get an idea of specific factors that are systematically associated with differences."
  },
  {
    "objectID": "blog/2024-06_meta-analysis/index.html#i-the-research-question",
    "href": "blog/2024-06_meta-analysis/index.html#i-the-research-question",
    "title": "Meta analysis for absolute beginners",
    "section": "I The research question",
    "text": "I The research question\nImagine our research question is: “Can people tell true news from false News”. And we know that there is psychological literature on that topic. These studies typically look like this:\n\n\n\n\n\nExample of items used by Pennycook et al. (2021)\n\n\n\n\n\n\nParticipants see a bunch of news headlines, sometimes with a lede and source, as they would appear on social media typically. Some of these headlines are false (typically identified as such by fact-checking sites such as Snopes), and some of them are true. For each news headline, they are asked to which extent they believe the headline to be accurate, (e.g. “To the best of your knowledge, how accurate is the claim in the above headline” 1 = Not at all accurate, 4 = Very accurate).\nIn order to do a meta-analysis, we need to have some quantifying measure for our research question. In this case, it is pretty straightforward. Researchers refer to people’s capacity to tell true news from false news as news discernment, which is simply:\n\\[\n\\text{discernment} = \\text{mean accuracy}_{\\text{true news}} - \\text{mean accuracy}_{\\text{false news}}\n\\]\n\n\n\nWe have a research question, and we also already have an idea of a quantifiable answer. At this point, we would ideally write up a pre-registration. This is basically a plan for our meta-analysis project, including the literature search and analysis plan, but a plan that we commit to making public.\nHowever, it is hard to write up a pre-registration for a meta-analysis if you don’t know anything about a meta-analysis. So we’ll skip this step for now and jump right into effect sizes."
  },
  {
    "objectID": "blog/2024-06_meta-analysis/index.html#ii-effect-sizes",
    "href": "blog/2024-06_meta-analysis/index.html#ii-effect-sizes",
    "title": "Meta analysis for absolute beginners",
    "section": "II Effect sizes",
    "text": "II Effect sizes\nIt might feel weird to talk about standardized effect sizes before the systematic literature search. But only if we know how to calculate (standardized) effect sizes, we can know what exactly we are looking for in articles.\nWe want to be able to compare discernment across studies. The problem is: different studies use different measurement scales. For example, Dias, Pennycook, and Rand (2020) use a 4-point scale, while Roozenbeek et al. (2020) use a 7-point scale. How can we compare the results of the two?\n\n\n\n\n\n\n\n\n\n\n\n\nref\nmean_accuracy_true\nmean_accuracy_fake\naccuracy_scale_numeric\ndiscernment\n\n\n\n\nDias_2020\n2.57\n1.67\n4\n0.90\n\n\nRoozenbeek_2020\n5.40\n2.49\n7\n2.91\n\n\n\n\n\nThe solution are standardized effect sizes. Instead of expressing discernment on the original scale of the study, we express discernment in terms of standard deviations. All we have to do is divide the discernment score by the (pooled) standard deviation of true and false news. So on top of the mean ratings, we also need the standard deviations.\n\n\n\n\n\nref\nmean_accuracy_true\nsd_accuracy_true\nmean_accuracy_fake\nsd_accuracy_fake\naccuracy_scale_numeric\ndiscernment\n\n\n\n\nDias_2020\n2.57\n0.52\n1.67\n0.42\n4\n0.90\n\n\nRoozenbeek_2020\n5.40\n1.62\n2.49\n1.34\n7\n2.91\n\n\n\n\n\nThe standardized discernment measure that we are after is generally referred to as a standardized mean difference (SMD), with perhaps the most popular version being called Cohen’s d, named after the psychologist and statistician Jacob Cohen.\nCohen’s d is calculate as\n\\[\n\\text{Cohen's d} = \\frac{\\bar{x}_{\\text{true}} - \\bar{x}_{\\text{false}}}{SD_{\\text{pooled}}}\n\\] with\n\\[\nSD_{\\text{pooled}} = \\sqrt{\\frac{SD_{\\text{true}}^2+SD_{\\text{false}}^2}{2}}\n\\]\nUsing these formula, we can now calculate our standardized discernment measure.\n\npooled_sd &lt;- function(sd_true, sd_false) {\n  sd_pooled &lt;- sqrt((sd_true^2 + sd_false^2) / 2)\n  return(sd_pooled)\n}\n\nmeta &lt;- meta %&gt;% \n  mutate(discernment = mean_accuracy_true - mean_accuracy_fake,\n         pooled_sd = pooled_sd(sd_accuracy_true, sd_accuracy_fake), \n         discernment_std = discernment/pooled_sd)\n\n\n\n\n\n\nref\nmean_accuracy_true\nmean_accuracy_fake\nsd_accuracy_fake\naccuracy_scale_numeric\nsd_accuracy_true\ndiscernment\npooled_sd\ndiscernment_std\n\n\n\n\nDias_2020\n2.57\n1.67\n0.42\n4\n0.52\n0.90\n0.4726521\n1.904149\n\n\nRoozenbeek_2020\n5.40\n2.49\n1.34\n7\n1.62\n2.91\n1.4866069\n1.957478\n\n\n\n\n\nGreat! We are almost ready to go to the next step and talk about meta-analytic models. But before that, there’s one thing missing still: the standard error (SE).\n\n\n\n\n\n\nNote\n\n\n\nStandard error (SE) and standard deviation (SD) are not the same thing. SD measures the variability observed within a sample. In our case, e.g. distribution accuracy ratings of fake news. It can be calculated directly from the data. SE describes the variability of a (hypothetical) distribution of an effect. In our case, e.g. a distribution of mean accuracy ratings of fake news, each based on a different sample. We can only guess this distribution based on the data by relying on statistical theory.\n\n\nThe SE is like a measure of certainty. How certain can we be that we observe a certain discernment effect in a study not only because of sampling variation (e.g. the specific sample of participants we asked), but because there is actually a true effect in the population (in all people)? The bigger the sample, the smaller the SE, i.e. the more certain we can be that whatever we observe is actually representative of the population we want to make conclusions on.\n\\[\nSE_{\\text{Cohen's d}} = SD_{\\text{pooled}}\\sqrt{\\frac{1}{n_\\text{true}}+\\frac{1}{n_\\text{false}}}\n\\]\nwith \\(n_\\text{false}\\) being the sample size of fake news items, \\(n_\\text{true}\\) the sample size of true news items in group 1, and \\(SD_{\\text{pooled}}\\) the pooled standard deviation of both groups (see above).\nIn our case, things are a bit tricky. Usually, in studies of news judgements, participants rate both false and true news items, and several of each category. First, this means that technically, we have to calculate the SE slightly differently. In the SE formula above, we were assuming independence of true and false news ratings. But the distributions of false and true news are necessarily correlated, because they are coming from the same participants. For this tutorial, we will ignore this. Second, our n are all instances of news ratings, i.e. is the product of the number of participants in our study (n_subj) and the number of news ratings per participant (n_news).\n\n\n\n\n\nref\nn_subj\nn_news\nn\n\n\n\n\nDias_2020\n187\n24\n4488\n\n\nRoozenbeek_2020\n700\n8\n5600\n\n\n\n\n\nWith this information, we can now calculate the SE. In our case, n is the combined number of rating instances. Let’s just assume that participant saw equally many false news as true news, so that we have \\(n_\\text{true} = n_\\text{false} = n/2\\)\n\nSE_Cohens_d &lt;- function(sd_pooled, n_true, n_false) {\n  se_d &lt;- sd_pooled * sqrt((1 / n_true) + (1 / n_false))\n  return(se_d)\n}\n\nmeta &lt;- meta %&gt;% \n  mutate(discernment_SE = SE_Cohens_d(sd_pooled = pooled_sd, n_true = n/2, n_false = n/2))\n\n\n\n\n\n\nref\npooled_sd\nn\ndiscernment_std\ndiscernment_SE\n\n\n\n\nDias_2020\n0.4726521\n4488\n1.904149\n0.0141106\n\n\nRoozenbeek_2020\n1.4866069\n5600\n1.957478\n0.0397312\n\n\n\n\n\nYay! Now that we have an effect size and its SE, we could run a meta-analysis.\nIf this was a bit tedious, here’s some good news: We do not have to do this by hand every time. There are several R packages specifically dedicated to meta-analysis that you can use. For example, the metafor package has a function called escalc() to calculate various effect sizes that are commonly used in meta-analyses. Here is an example of how to do what we just did using escalc:\n\nlibrary(metafor)\n\n# Calculate SMD and its variance\nmeta_effect_sizes &lt;- escalc(measure = \"SMD\", \n                  # diff = true (m1i) - fake (m2i)\n                  m1i=mean_accuracy_true,\n                  sd1i=sd_accuracy_true, \n                  m2i= mean_accuracy_fake, \n                  sd2i=sd_accuracy_fake, \n                  n1i = n_observations/2,\n                  n2i = n_observations/2, \n                  data = meta)\n\n# View the resulting data frame with effect sizes and variances\nmeta_effect_sizes %&gt;% \n  select(ref, starts_with(\"mean\"), yi, vi) %&gt;% \n  mutate(SE = sqrt(vi)) %&gt;% \n  kable()\n\n\n\n\n\n\n\n\n\n\n\n\nref\nmean_accuracy_fake\nmean_accuracy_true\nyi\nvi\nSE\n\n\n\n\nDias_2020\n1.67\n2.57\n1.903831\n0.0012928\n0.0359551\n\n\nRoozenbeek_2020\n2.49\n5.40\n1.957216\n0.0010563\n0.0325010\n\n\n\n\n\nIn this output, yiis the SDM (i.e. our standardized discernment effect), and vi is the variance (which is just \\(SE^2\\)). The values for SE are not exactly the same we obtained when calculating Cohen’s D by hand before, because escalc() uses not Cohen’s D, but Hedges’ G (a very similar measure but with some small sample correction).\nThere are many more standardized effect sizes. Which one you want to use depends mostly on the kind of data you want to analyze. For dichotomous outcomes, researchers typically use (log) odds or risk ratios, for associations between two continuous variables simply a correlation1 and so on. There is a great introduction to doing meta-analysis by Harrer et al. (2021). The book is freely available online and includes r-code. There is a whole chapter on effect sizes which you might want to look at.\nWe know have an idea of what we need to extract from the papers, in order to be able to run a meta-analysis:\n\nthe means and standard deviations of false and true news ratings, respectively.\nthe number of participants and the number of news ratings per participant\n\nWith that in mind, we are ready to dive into the literature search."
  },
  {
    "objectID": "blog/2024-06_meta-analysis/index.html#iii-systematic-literature-search",
    "href": "blog/2024-06_meta-analysis/index.html#iii-systematic-literature-search",
    "title": "Meta analysis for absolute beginners",
    "section": "III Systematic literature search",
    "text": "III Systematic literature search\nIdeally, we want all studies that have ever been written on our research question. Remember, the more the better. There’s just one issue: try to type something like “misinformation” or “fake news” in google scholar.\n\n\n\nScreenshot from a google scholar search on June 27, 2024\n\n\nUps. Not enough time to review all these in a life time probably. The first thing we should do is be a bit more specific in what we’re looking for. So let’s refine our search string.\n\nSearch string\nYou will most likely never start a systematic literature review having absolutely no clue about the topic you care about. There might at least be that one paper that inspired your idea for a meta-analysis. This paper gives you some first ideas of keywords that you could look for.\nIn our case, we settled on this search string\n\n‘“false news” OR “fake news” OR “false stor*” AND “accuracy” OR “discernment” OR “credibilit*” OR “belief” OR “susceptib*”’\n\nThere are a couple of things here. We used ’OR’s and ’AND’s to combine words. These are Boolean operators. Quick quizz:\n\n\n\nImagine we would have run a title-search (i.e. not searching abstracts or other content, but only the headlines of articles). Would this search string have yielded a study called “News accuracy ratings of US adults during 2016 presidential elections”?\n\n\n\n\n\n\n\nBoolean AND operator. Only if both keywords are included, a result will show up.\n\n\n\n\n\n\nBoolean OR operator. As long as one of the keywords is included, a result will show up.\n\n\n\n\nNo. And that is because of the boolean operators. The “OR” operator tells the search engine that as long as one of the keywords is matched, return the result. By contrast, the AND operator tells the search engine, that both keywords need to be matched. For example, “accuracy” OR “discernment” would return an article entitled “News accuracy ratings of US adults during 2016 presidential elections”, whereas “accuracy” AND “discernment” would not.\nInstead of single keywords, you can also link combinations of keywords. Our search string, put a bit more abstractly, reads …OR…OR…AND…Or…OR… As in math, there is a hierarchy among operators. On Scopus (the search engine we used), OR operators are treated before AND operators. It’s like in math, when you know that \\(1 + 2*4 = 9\\), because you multiply before you add. You could re-write the math term \\(1 + (2*4) = 9\\) to make the convention explicit, and you could do the same for the search string (…OR…OR…) AND (…Or…OR…), but you don’t have to.\nImagine that we are happy with the combination of keywords that we have in our search string. Where do we look for articles now?\n\n\nData bases\nYou will all know Google Scholar, which is very convenient for quick searches. We all use it regularly to find a specific article that we are looking for, or to get a first impression of results from some keywords.\nThe one big disadvantage for google scholar is that the results (and least in terms of order) are user-specific. You and I running the same search query on google scholar, results will not be listed in the same way. This is problematic, because literature of a systematic review should ideally be reproducible.\nLuckily, there are loads of other data bases that you can search. Some of them are issue specific (e.g. Pubmed for medicine in the broadest sense) while others are more general (e.g. Scopus, Web of Science).\n\n\n\n\n\n\nNote\n\n\n\nMost of these databases will miss non-published data. Researchers still have a hard time of publishing null findings, and very recent pre-prints have not yet made it through peer-review. Only searching peer-reviewed, published records might therefor bias your selection of studies. To counteract, you can additionally search on pre-print servers, e.g. PsyArXiv for psychology.\n\n\nIn our case, say we want to search Scopus, a very large and general data base.\n\nWe can enter our search term, and specify what parts of the articles we want to search for. We will pick Article title, Abstract & Keywords.\nOnce we click on search, we see the number of overall search results, and a list of articles.\n\nThe number of results is less shocking than the initial one from searching “misinformation” on Google Scholar. But it is still a whole lot of articles, and we couldn’t possibly read all of them entirely.\nWe can use some more refined criteria to filter out some likely irrelevant results2. In the left hand panel, we can select for example the document type, language, or subject areas. We can exclude, for example, a couple of disciplines that are most likely not relevant, such as “Chemistry”.\n\n\n\n\n\n\n\nThe more restrictions we make, the more it will reduce the number of search results. You will also not that adding these restrictions changes the search string (you can see the full string by clicking on the orange “Advanced query” button).\n\nIf we’re finally happy with our search, we need to save the search results somewhere3. Some people like to export all results to a reference manager such as Zotero. For our project, I preferred not to flood my Zotero and use the convenient options to export search results to a .csv file.\n\n\n\n\n\nBoolean AND operator. Only if both keywords are included, a result will show up.\n\n\n\n\n\n\nBoolean OR operator. As long as one of the keywords is included, a result will show up.\n\n\n\n\n\n\n\nTime to run your own literature search:\n\nFind a partner\nMake up a search string\nRun it on Scopus\nRestrict search results\nDownload a .csv file with the first 100 results\n\n\n\nNow that we have all results stored, there are still many, many articles. Do we have to read them all? No. Before we start reading entire papers, we do screening.\n\n\nScreening\nThere are usually two stages of screening: (i) title screening and (ii) abstract screening. Since screening decisions can sometimes be quite arbitrary, we ideally want several researchers to do it independently.\n\nTitle screening\nDuring title screening we throw out titles that are very obviously not relevant.\n\n\n\nDo your:\n\nStay with your partner\nUpload your .csv search result file in a google spreadsheet\nLimit your spreadsheet to the first 30 results.\nMake a new tab and copy only the title column. Make a second column called “reviewer”. Make a third column “decision”. Duplicate this tab.\nEach reviewer screens all titles independently. Put your respective initals in all cells of the “reviewer column”. Agree on a code for in/exclusion in the “decision” column.\nCompare with your partner and explain cases of exclusion\n\n\n\nThere will almost certainly be some cases where you and your co-reviewer will not agree.4.\nOne, conservative solution to proceed is to only remove titles that all reviewers found to be irrelevant, and make all others pass through to the next screening stage.\n\n\nAbstract screening\nThe next stage, abstract screening, is a bit more challenging. At this point, the any search result that we retained might be relevant to review for us. That means, from now on, we really need to justify why we include some articles, but not others.\nHere are some criteria that we can set:\n\n\n\n\n\n\n\n\n\n\nCriterionID\nCriterion_type\nDefinition\n\n\n\n\n1.0\nDocument type\nAll literature with original data: peer reviewed papers, but also pre-prints (this implies unique data that is not otherwise already included in the analysis)\n\n\n2.0\nResearch question\nis about the perceived accuracy of fake AND true news\n\n\n3.0\nAccess\nis the paper available\n\n\n4.0\nMethod\nmeasures accuracy or an equivalent construct (e.g. trustworthiness, truthfulness)\n\n\n4.1\nMethod\nprovides all relevant summary statistics (i.e. accuracy ratings for both fake AND true news) or data to produce them; a mere discernment score is not enough\n\n\n4.2\nMethod\nprovides a clear control condition that is comparable with those of other studies\n\n\n5.0\nStimuli\nreal world news only\n\n\n\n\n\nThese inclusion5 criteria are very important. For every article that we reject after the title screening, we use them to justify why we excluded them.\n\n\n\n\n\n\nNote\n\n\n\nThe restrictions we made on our data base are technically also inclusion criteria. They are irrelevant for the screening, because they were already used to filter the results. But you need to report them.\n\n\nInclusion criteria might be evolving during your screening process. Most likely, you weren’t completely clear on everything you want or not, before launching the literature search. That’s fine, as long as you are transparent about it (see section of pre-registration).\n\n\n\nHere is a google spreadsheet with 10 of abstracts to screen. With you partner:\n\nDownload the .csv or make a copy in Google Spreadsheets\nRead the abstracts.\nFor each abstract, give your inclusion decision. If you reject, justify.\nCompare your decisions\n\n\n\n\n\n\nFull text assessment\nNow, finally, it is time to read full articles.\n\n\n\nWithin your groups:\n\nPick one of these two articles each, i.e. one per person.\n\n\nDias, N., Pennycook, G., & Rand, D. G. (2020). Emphasizing publishers does not effectively reduce susceptibility to misinformation on social media. Harvard Kennedy School Misinformation Review. https://doi.org/10.37016/mr-2020-001\nRoozenbeek, J., Schneider, C. R., Dryhurst, S., Kerr, J., Freeman, A. L. J., Recchia, G., van der Bles, A. M., & van der Linden, S. (2020). Susceptibility to misinformation about COVID-19 around the world. Royal Society Open Science, 7(10), 201199. https://doi.org/10.1098/rsos.201199\n\n\nAccess the articles via the usual means you find and download papers.\nThink about whether you include them.\nExtract the information you need.\n\n\n\n\n\nPre-registration\nIdeally, you should already preregister after you are clear on your research question and effect sizes. There are some good templates, e.g. on the Open Science Framework (OSF).\n\n\nPRISMA\nWhat we just did follows pretty closely the PRISMA guidelines. Once you are done with your systematic review, you should use their overview template to communicate your literature review and its different stages."
  },
  {
    "objectID": "blog/2024-06_meta-analysis/index.html#iv-meta-analysis",
    "href": "blog/2024-06_meta-analysis/index.html#iv-meta-analysis",
    "title": "Meta analysis for absolute beginners",
    "section": "IV Meta-analysis",
    "text": "IV Meta-analysis\nHere’s an intuitive way to think about a meta-analysis: We just take the effect sizes of all the studies and then calculate an some average. That average, we hope, is closer to the “true effect” than any of the single studies.\nThe problem is: Some studies have a way larger sample than others. Our average should take this into account–in other words, what we want is a weighted average. The bigger the sample–and thus the smaller the standard error–the more weight a study should have.\n\nFixed-effect vs. Random-effect model\nThe way we calculate these weights is where things become a bit philosophical. There are two models to conceive of our study effect sizes: (i) a fixed effect model and (ii) a random effects model. The fixed-effect model assumes that there is one “true” effect only. Effect sizes vary from one study to another, but only becasue sampling variation. The random-effects model assumes that there are several “true” effects. Studies vary because of sampling variation, but also because they have different “true” effects.\nIf this sounds confusing, you’re not alone. I like to imagine it like this: Imagine you run the same study on news-discernment both in the US and in China. You get very different results. According to the fixed-effect model, these are two samples from the same population–all people–and there is one true value for news discernment capacity across the globe. According to the random-effects model, there these are two different populations, who have two distinct true values for news discernment capacity.\nIf this still isn’t clear (I wouldn’t blame you), you can give this chapter a read. The take away is that for many meta-analyses (at least in social sciences), a random-effects model is probably the better choice.\n\n“[…] there may be countless reasons why real differences exist in the true effect sizes of studies. The random-effects model addresses this concern. It provides us with a model that often reflects the reality behind our data much better.” (Harrer et al. 2021)\n\n\n\nWeighted averages\nRemember that our meta-analysis ultimately leaves us with some sort of weighted average. We want studies with larger samples (and thus smaller SEs, or sampling variation) to have more weight.\nThe most common approach to do this is often called inverse-variance weighting or simply inverse-variance meta-analysis (Harrer et al. 2021).\n\\[\nw_k = \\frac{1}{s^2_k}\n\\] where \\(w_k\\) is the weight for each study \\(k\\) and \\(s^2_k\\) is the variance (the squared standard error) of each effect size.\nBased on these weights, the meta-analytic average is calculated as the sum of weighted effect sizes divided by the sum o fall individual weights\n\\[\n\\hat\\theta = \\frac{\\sum^{K}_{k=1} \\hat\\theta_kw_k}{\\sum^{K}_{k=1} w_k}\n\\]\nwith \\(\\hat\\theta\\) estimate of the true pooled effect and \\(\\hat\\theta_k\\) each study’s effect size.\nFor the random effects model, it’s a little more difficult. It assumes that on top of the sampling variation, there is also other “random” variance \\(\\tau^2\\), or tau-squared.\n\\[\nw^*_k = \\frac{1}{s^2_k+\\tau^2}\n\\] Estimating \\(\\tau^2\\) is possible but but complex. Luckily, there are packages that do this for us. After determining the correct weight, random-effects models typically also use the same approach of inverse variance method as described above.\nLet’s try an example of how to do a random-effects meta-analysis with the metafor package. Before we can run a meta-analysis, we need to make sure to have a data-frame with effect sizes.\n\n\n\nHere is a spreadsheet.\n\nDownload it.\nCalculate effect sizes using the escalc() function from the metafor package.\n\n\n\n\nlibrary(metafor)\n\nmeta &lt;- read_csv(\"data/meta_randomeffects_example.csv\")\n\n# Calculate SMD and its variance\nmeta_effect_sizes &lt;- escalc(measure = \"SMD\", \n                  # diff = true (m1i) - fake (m2i)\n                  m1i=mean_accuracy_true,\n                  sd1i=sd_accuracy_true, \n                  m2i= mean_accuracy_fake, \n                  sd2i=sd_accuracy_fake, \n                  n1i = n/2,\n                  n2i = n/2, \n                  data = meta) %&gt;% \n  arrange(yi)\n\n# View the resulting data frame \nmeta_effect_sizes %&gt;% \n  kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npaperID\nref\nmean_accuracy_fake\nmean_accuracy_true\nsd_accuracy_fake\nsd_accuracy_true\nn\nyi\nvi\n\n\n\n\n7\nGuess_2020\n2.40\n2.77\n0.729\n0.694\n19638\n0.5198510\n0.0002106\n\n\n1\nAltay_2022\n1.99\n2.71\n0.890\n0.880\n2990\n0.8133421\n0.0014484\n\n\n2\nLutzke_2019\n4.28\n6.38\n2.550\n2.340\n5604\n0.8579899\n0.0007795\n\n\n10\nLuo_2022\n2.98\n4.72\n1.950\n2.010\n1150\n0.8781128\n0.0038135\n\n\n4\nPennycook_2020\n1.95\n2.64\n0.600\n0.530\n4020\n1.2186749\n0.0011797\n\n\n8\nPennycook_2020_b\n0.33\n0.65\n0.280\n0.220\n12795\n1.2708078\n0.0003757\n\n\n9\nClayton_2020\n1.96\n3.34\n0.934\n0.927\n4221\n1.4827995\n0.0012081\n\n\n3\nPennycook_2018\n1.78\n2.83\n0.600\n0.700\n2847\n1.6102018\n0.0018603\n\n\n6\nPennycook_2019\n1.66\n2.59\n0.450\n0.440\n63456\n2.0897310\n0.0000974\n\n\n5\nBronstein_2019\n1.79\n2.78\n0.460\n0.470\n12048\n2.1287766\n0.0005201\n\n\n\n\n\nWith the effect sizes, we can now run our meta-analysis.\n\nresult &lt;- metafor::rma(yi, vi, data = meta_effect_sizes, slab = ref)\nmetafor::forest(result)\n\n\n\n\n\n\n\n\nYay. This plot is called a “forest plot”. We can see all effect sizes of the the studies that we fed into the model. In the right en column we see all standardizes discernment effect sizes and their confidence intervals. In the bottom row, we see the row label “RE Model”, for random-effects model, the value for our meta-analytic average.\n\n\nMulti-level\nSo far so good. Things get more complicated if we have more than one effect size per study, and perhaps even per sample within a study. For our studies, this is the case.\nOne of the reasons is that participants did not only see several news items, but sometimes also news items of different categories, such as politically concordant (e.g. pro-republican news for a Republican participant) and discordant (e.g. pro-democrat news for a Republican participant).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npaperID\nref\nsampleID\nnews_family\nn\nyi\nvi\n\n\n\n\n41\nEun-Ju_2023\n3\npolitically_discordant\n320.00\n-0.2660372\n0.0126106\n\n\n41\nEun-Ju_2023\n12\npolitically_concordant\n400.00\n-0.2505704\n0.0100785\n\n\n41\nEun-Ju_2023\n6\npolitically_concordant\n320.00\n-0.1423425\n0.0125317\n\n\n41\nEun-Ju_2023\n9\npolitically_discordant\n400.00\n0.0000000\n0.0100000\n\n\n41\nEun-Ju_2023\n9\npolitically_concordant\n400.00\n0.0854359\n0.0100091\n\n\n69\nShirikov_2024\n3\npolitically_concordant\n72880.48\n0.1293124\n0.0000550\n\n\n56\nGarrett_2021\n1\npolitically_concordant\n2212.00\n0.1371033\n0.0018126\n\n\n41\nEun-Ju_2023\n6\npolitically_discordant\n320.00\n0.1396615\n0.0125305\n\n\n69\nShirikov_2024\n3\npolitically_discordant\n72880.48\n0.1771236\n0.0000551\n\n\n41\nEun-Ju_2023\n12\npolitically_discordant\n400.00\n0.1784714\n0.0100398\n\n\n69\nShirikov_2024\n4\npolitically_discordant\n14625.28\n0.2161790\n0.0002751\n\n\n69\nShirikov_2024\n4\npolitically_concordant\n14625.28\n0.2523929\n0.0002757\n\n\n58\nLyons_2024\n1\npolitically_discordant\n4848.00\n0.2843118\n0.0008334\n\n\n58\nLyons_2024\n1\npolitically_concordant\n4842.00\n0.3126600\n0.0008362\n\n\n58\nLyons_2024\n3\npolitically_discordant\n4701.00\n0.3145489\n0.0008614\n\n\n58\nLyons_2024\n3\npolitically_concordant\n4692.00\n0.3325030\n0.0008643\n\n\n64\nGuess_2024\n5\npolitically_concordant\n810.00\n0.3376436\n0.0050086\n\n\n55\nGawronski_2023\n2\npolitically_discordant\n8850.00\n0.3398163\n0.0004585\n\n\n58\nLyons_2024\n1\npolitically_discordant\n4788.00\n0.3517364\n0.0008483\n\n\n64\nGuess_2024\n5\npolitically_discordant\n810.00\n0.3631048\n0.0050197\n\n\n\n\n\nAs a result, for each sample, we have two observations, i.e two lines in our data frame. Because observations are not independent of each other, and a standard meta-analysis assumes independence, we have to find a way of modelling this dependency. This is where multi-level meta-analysis becomes relevant.\nIn fact, the standard random-effects model that we’ve learned about already is a multi-level model. It has two levels of variation: the sampling variance (the SE), and some unique study variance (what we labeled ‘tau’). Now we want to add an additional level between these two, namely some unique sample variance.\nThere is a great chapter on multi-level models in Harrer et al. (2021). The key take-away is that as soon as there are dependencies in our data frame, we need to model them. If effect sizes are correlated and we do not model the sources of that correlation, this can artificially reduce the heterogeniety of our effect sizes and lead to false-positive results (Harrer et al. 2021). For example, if we run ten studies but 8 on them rely on the same 100 participants, we might find results that are very similar across studies, but that’s precisely because they can hardly be seen as independent studies.\nImplementing a multilevel meta-analysis with the metafor package in R is pretty intuitive, if you have the habit of running linear mixed-models. The syntax is very similar.\nFirst, we crate add an observation_id to our data frame, which identifies each row, i.e. each effect size in our sample. We then create a unique_sample_id, which is a combination of the ref and the sampleID variable.\n\nmeta &lt;- meta %&gt;% \n  mutate(observation_id = 1:nrow(.), \n         unique_sample_id = paste0(ref, sampleID))\n\nhead(meta) %&gt;% \n  kable() %&gt;%\n  column_spec(c(8, 9), background = \"lightblue\")\n\n\n\n\npaperID\nref\nsampleID\nnews_family\nn\nyi\nvi\nobservation_id\nunique_sample_id\n\n\n\n\n41\nEun-Ju_2023\n3\npolitically_discordant\n320.00\n-0.2660372\n0.0126106\n1\nEun-Ju_20233\n\n\n41\nEun-Ju_2023\n12\npolitically_concordant\n400.00\n-0.2505704\n0.0100785\n2\nEun-Ju_202312\n\n\n41\nEun-Ju_2023\n6\npolitically_concordant\n320.00\n-0.1423425\n0.0125317\n3\nEun-Ju_20236\n\n\n41\nEun-Ju_2023\n9\npolitically_discordant\n400.00\n0.0000000\n0.0100000\n4\nEun-Ju_20239\n\n\n41\nEun-Ju_2023\n9\npolitically_concordant\n400.00\n0.0854359\n0.0100091\n5\nEun-Ju_20239\n\n\n69\nShirikov_2024\n3\npolitically_concordant\n72880.48\n0.1293124\n0.0000550\n6\nShirikov_20243\n\n\n\n\n\nWe can then proceed and run our multi-level model.\n\n# Multilevel random effect model for accuracy\nmultilevel_model &lt;-  metafor::rma.mv(yi, vi, random = ~ 1 | unique_sample_id / observation_id, data=meta)\n\nOne additional thing, which I will not further discuss unfortunately, but which you should know about (you can look here for more info): Multi-level models do not account for dependencies in sampling error. But when one same sample contributes several effect sizes, we should expect their respective sampling errors to be correlated (Harrer et al. 2021), too. To account for dependency in sampling errors, you should compute cluster-robust standard errors.\n\nrobust_multilevel_model &lt;- metafor::robust(multilevel_model, cluster = unique_sample_id)\n\n\n\nMeta-regression\nIn meta-analyses, we often want to go beyond mere averages. We also like to know what factors contribute to the differences we observe between effect sizes. Meta-regressions allow us to do exactly that.\nThe good new is that this is pretty easy to implement and understand (at after having worked our brains to try to understand multi-level models). It works just like any normal regression analysis.\nLet’s say we want to know differences in discernment associated with political concordance. All we have to do is add a term for that variable to our model.\n\n# Multilevel random effect model for accuracy\nmeta_regression_model &lt;-  metafor::rma.mv(yi, vi, \n                                          # you can also add multiple covariates\n                                          mods = ~news_family, \n                                          random = ~ 1 | unique_sample_id / observation_id, \n                                          data=meta)\n\nmodelsummary::modelsummary(meta_regression_model, \n                           stars = TRUE)\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                 \n                (1)\n              \n        \n        + p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\n        \n                \n                  intercept                        \n                  0.595***\n                \n                \n                                                   \n                  (0.082) \n                \n                \n                  news_familypolitically_discordant\n                  0.078*  \n                \n                \n                                                   \n                  (0.039) \n                \n                \n                  Num.Obs.                         \n                  86      \n                \n                \n                  AIC                              \n                  38.4    \n                \n                \n                  BIC                              \n                  48.2    \n                \n        \n      \n    \n\n\n\n\n\n\nHow do you interpret this regression table?"
  },
  {
    "objectID": "blog/2024-06_meta-analysis/index.html#publication-bias",
    "href": "blog/2024-06_meta-analysis/index.html#publication-bias",
    "title": "Meta analysis for absolute beginners",
    "section": "Publication bias",
    "text": "Publication bias\nTo be done."
  },
  {
    "objectID": "blog/2024-06_meta-analysis/index.html#limits",
    "href": "blog/2024-06_meta-analysis/index.html#limits",
    "title": "Meta analysis for absolute beginners",
    "section": "Limits",
    "text": "Limits\nDo be done. But most importantly, don’t compare apples and oranges in your meta-analysis (see this Data colada post)."
  },
  {
    "objectID": "blog/2024-06_meta-analysis/index.html#ressources",
    "href": "blog/2024-06_meta-analysis/index.html#ressources",
    "title": "Meta analysis for absolute beginners",
    "section": "Ressources",
    "text": "Ressources\nI’ve already mentioned it many times throughout this post, but Harrer et al. (2021) is a really great open-access resource. You can find the book online here.\n\nHarrer, M., Cuijpers, P., Furukawa, T.A., & Ebert, D.D. (2021). Doing Meta-Analysis with R: A Hands-On Guide. Boca Raton, FL and London: Chapman & Hall/CRC Press. ISBN 978-0-367-61007-4.\n\nAnother recommendation would be this introductory chapter by Daniel Lakens.\n\nLakens, D. (2022). Improving Your Statistical Inferences. Retrieved from https://lakens.github.io/statistical_inferences/. https://doi.org/10.5281/zenodo.6409077"
  },
  {
    "objectID": "blog/2024-06_meta-analysis/index.html#footnotes",
    "href": "blog/2024-06_meta-analysis/index.html#footnotes",
    "title": "Meta analysis for absolute beginners",
    "section": "Footnotes",
    "text": "Footnotes\n\n\noften researchers use a z-transformed version of a correlation coefficient↩︎\nthese are part of in-(or ex-)clusion criteria, but let’s leave that for later↩︎\nOn scopus, you can additionally save your search history and results by making an account.↩︎\nThere are ways to quantify the agreement between raters (e.g. Cohen’s kappa), but we don’t bother much about that here.↩︎\nor exclusion, depending on how you want to frame it↩︎"
  },
  {
    "objectID": "blog/2024-11_learning-R/index.html",
    "href": "blog/2024-11_learning-R/index.html",
    "title": "Learning Ressources",
    "section": "",
    "text": "There are plenty of free online resources to help learn R and git. Here is a quick collection of some that I came across."
  },
  {
    "objectID": "blog/2024-11_learning-R/index.html#footnotes",
    "href": "blog/2024-11_learning-R/index.html#footnotes",
    "title": "Learning Ressources",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nthis book is mostly about statistics but uses R for examples↩︎"
  },
  {
    "objectID": "research/index.html",
    "href": "research/index.html",
    "title": "Research",
    "section": "",
    "text": "Richardson, E., Davis, M., Isaac, & Keil, F. (2025). Agenda setting and the emperor’s new clothes: People diagnose information cascades during sequential testimony by reasoning about informants’ speaking order and social status. Open Mind. https://doi.org/10.31234/osf.io/vkgz5 ( PDF |  code & data)\nRichardson, E., Hok, H., Shaw, A., & Keil, F. (2025). Herding cats: Children and adults infer collective decision speed from team size and diversity, but disagree about whether consensus strength matters more than team size. Cognition, 263, 106211. https://doi.org/10.1016/j.cognition.2025.106211 ( Manuscript |  code & data)\nMenendez, D., Richardson, E., McNeil, K., & Gelman, S. (2024). Discovering the world of viruses: Testing the influence of anthropomorphic representations on children’s learning about COVID-19. Developmental Psychology, 61, 513–529. https://doi.org/10.1037/dev0001882 ( PDF |  code & data)\nRichardson, E., & Keil, F. (2022). The potential for effective reasoning guides children’s preference for small group discussion over crowdsourcing. Scientific Reports, 12, 1193. https://doi.org/10.1038/s41598-021-04680-z ( PDF |  code & data)\nRichardson, E., & Keil, F. (2022). Thinking takes time: Children use agents’ response times to infer the source, quality, and complexity of their knowledge. Cognition, 224. https://doi.org/10.1016/j.cognition.2022.105073 ( PDF |  code & data)\nRichardson, E., Sheskin, M., & Keil, F. (2021). An illusion of self‐sufficiency for learning about artifacts in scaffolded learners, but not observers. Child Development, 92, 1523–1538. https://doi.org/https://doi.org/10.1111/cdev.13506 ( PDF |  code & data)"
  },
  {
    "objectID": "research/index.html#peer-reviewed-publications",
    "href": "research/index.html#peer-reviewed-publications",
    "title": "Research",
    "section": "",
    "text": "Richardson, E., Davis, M., Isaac, & Keil, F. (2025). Agenda setting and the emperor’s new clothes: People diagnose information cascades during sequential testimony by reasoning about informants’ speaking order and social status. Open Mind. https://doi.org/10.31234/osf.io/vkgz5 ( PDF |  code & data)\nRichardson, E., Hok, H., Shaw, A., & Keil, F. (2025). Herding cats: Children and adults infer collective decision speed from team size and diversity, but disagree about whether consensus strength matters more than team size. Cognition, 263, 106211. https://doi.org/10.1016/j.cognition.2025.106211 ( Manuscript |  code & data)\nMenendez, D., Richardson, E., McNeil, K., & Gelman, S. (2024). Discovering the world of viruses: Testing the influence of anthropomorphic representations on children’s learning about COVID-19. Developmental Psychology, 61, 513–529. https://doi.org/10.1037/dev0001882 ( PDF |  code & data)\nRichardson, E., & Keil, F. (2022). The potential for effective reasoning guides children’s preference for small group discussion over crowdsourcing. Scientific Reports, 12, 1193. https://doi.org/10.1038/s41598-021-04680-z ( PDF |  code & data)\nRichardson, E., & Keil, F. (2022). Thinking takes time: Children use agents’ response times to infer the source, quality, and complexity of their knowledge. Cognition, 224. https://doi.org/10.1016/j.cognition.2022.105073 ( PDF |  code & data)\nRichardson, E., Sheskin, M., & Keil, F. (2021). An illusion of self‐sufficiency for learning about artifacts in scaffolded learners, but not observers. Child Development, 92, 1523–1538. https://doi.org/https://doi.org/10.1111/cdev.13506 ( PDF |  code & data)"
  },
  {
    "objectID": "research/index.html#large-collaborations",
    "href": "research/index.html#large-collaborations",
    "title": "Research",
    "section": "Large Collaborations",
    "text": "Large Collaborations\n\nCologna, V., Mede, N. G., Berger, S., Besley, J., Brick, C., Joubert, M., Maibach, E. W., Mihelj, S., Oreskes, N., Schäfer, M. S., Linden, S. van der, Abdul Aziz, N. I., Abdulsalam, S., Shamsi, N. A., Aczel, B., Adinugroho, I., Alabrese, E., Aldoh, A., Alfano, M., … Zwaan, R. A. (2025). Trust in scientists and their role in society across 68 countries. Nature Human Behaviour, 1–18. https://doi.org/10.1038/s41562-024-02090-5 ( preprint)\nMede, N. G., Cologna, V., Berger, S., Besley, J., Brick, C., Joubert, M., Maibach, E. W., Mihelj, S., Oreskes, N., Schäfer, M. S., Van Der Linden, S., Abdul Aziz, N. I., Abdulsalam, S., Shamsi, N. A., Aczel, B., Adinugroho, I., Alabrese, E., Aldoh, A., Alfano, M., … Zwaan, R. A. (2025). Perceptions of science, science communication, and climate change attitudes in 68 countries  the TISP dataset. Scientific Data, 12(1), 114. https://doi.org/10.1038/s41597-024-04100-7 ( preprint)"
  },
  {
    "objectID": "research/index.html#conference-proceedings",
    "href": "research/index.html#conference-proceedings",
    "title": "Research",
    "section": "Conference Proceedings",
    "text": "Conference Proceedings\n\npfanderHowWiseCrowd2024?( pdf | poster |  code & data)"
  },
  {
    "objectID": "research/index.html#working-papers",
    "href": "research/index.html#working-papers",
    "title": "Research",
    "section": "Working papers",
    "text": "Working papers\n\nPfänder, J., Kerzreho, L., & Mercier, H. (2024). Quasi-universal acceptance of basic science in the US. https://doi.org/10.31219/osf.io/qc43v (submitted)\nPfänder, J., Rouilhan, S. D., & Mercier, H. (2025). Trusting but forgetting impressive science. https://doi.org/10.31219/osf.io/argq5_v1 (submitted)\nPfänder, J., & Mercier, H. (2025). The french trust more the sciences they perceive as precise and consensual. https://doi.org/10.31219/osf.io/k9m6e_v1"
  },
  {
    "objectID": "research/index.html#talks",
    "href": "research/index.html#talks",
    "title": "Research",
    "section": "Talks",
    "text": "Talks\n\nPfänder, J., & Mercier, H. (2025, March). Trust in science. Paris School of International Affairs (Sciences Po), Paris.\nPfänder, J., & Altay, S. (2025, March). Spotting False News and Doubting True News. A Meta-Analysis of News Judgments. Infox sur seine, Paris.\nPfänder, J. (2024, July). How wise is the crowd: Can we infer people are accurate and competent merely because they agree with each other? 46th Annual Meeting of the Cognitive Science Society, Rotterdam.\nPfänder, J. (2024, June). Processus sous-tendant la confiance dans la science. 5ème édition des Rencontres Jeunes Chercheur·euse·s, Grenoble.\nPfänder, J., & Altay, S. (2024, February). Spotting False News and Doubting True News: A Meta-Analysis of News Judgments. EDMO Scientific Conference on Disinformation, Amsterdam."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Hello there!",
    "section": "",
    "text": "Hi! I’m an applied data scientist. I build experiments and statistical models to find the difference-that-makes-a-difference in behavior, beliefs, and decisons.\nThings I get excited about: data, puzzles, theory, statistical methods, good writing, philosophy of science, social network topologies & their influence on cumulative culture, learning strategies based on conformity / Condorcet methods / “the wisdom of crowds”, the spread of (mis)information, languages, and funky bass. Also kettlebells.\npast: postdoc @ UMich | PhD @ Yale | MAPSS @ UChicago RPCV @ US Peace Corps | Philosophy & History of Science @ stjohnscollege"
  },
  {
    "objectID": "blog/2024-09_tracked-changes/index.html",
    "href": "blog/2024-09_tracked-changes/index.html",
    "title": "Tracked changes for revisions",
    "section": "",
    "text": "During revisions, reviewers as us to make changes to your initially submitted manuscript. For reviewers to see what exactly you’ve changed in the manuscript, it is nice to have a version with tracked changes of your revised manuscript (kind of like the correction mode in Word). Luckily, there are tools available to make this very easy. I’ll quickly describe my workflow below."
  },
  {
    "objectID": "blog/2024-09_tracked-changes/index.html#generating-a-manuscript-with-papaja",
    "href": "blog/2024-09_tracked-changes/index.html#generating-a-manuscript-with-papaja",
    "title": "Tracked changes for revisions",
    "section": "Generating a manuscript with Papaja",
    "text": "Generating a manuscript with Papaja\nI tend to write manuscripts in RMarkdown, using the Papaja package. Render the .Rmd file to .pdf produces a .tex version. When first submitting the manuscript to a journal, I save the this .tex file and call it something like initial_submission.tex."
  },
  {
    "objectID": "blog/2024-09_tracked-changes/index.html#making-changes-requested-by-reviewers",
    "href": "blog/2024-09_tracked-changes/index.html#making-changes-requested-by-reviewers",
    "title": "Tracked changes for revisions",
    "section": "Making changes requested by reviewers",
    "text": "Making changes requested by reviewers\nAfter reviewers provided their feedback (fingers crossed it’s positive and did not lead to a rejection), I make all sort of changes to the paper, i.e. the original Rmd file. Once I’m done, I’ll render the file to .pdf again, in order to send back the revised manuscript. I’ll call the .tex file that comes along with it revised.tex."
  },
  {
    "objectID": "blog/2024-09_tracked-changes/index.html#tracking-changes",
    "href": "blog/2024-09_tracked-changes/index.html#tracking-changes",
    "title": "Tracked changes for revisions",
    "section": "Tracking changes",
    "text": "Tracking changes\nIn addition to my revised manuscript, I like to send reviewers a version where all the changes are immediately visible. To do so, I use the online Latex Diff Tool. All I need to do is copy-paste the content of my initial_submission.tex and my revised.tex file in the respective boxes. The tool then generates a version that highlights the changes. All I have to do is copy-paste that version into an empty .tex file and render it to .pdf. I always use Overleaf for that."
  },
  {
    "objectID": "teaching/index.html",
    "href": "teaching/index.html",
    "title": "Teaching",
    "section": "",
    "text": "Epistémologie et Esprit Critique (or, Introduction to research design)\n\n\n\nresearch design\n\nstatistics\n\nsimulation\n\nR\n\nggplot\n\n\n\nA hands-on introduction to research_design for students of the Sciences Pour un Monde Durable (SMD) undergraduate program at Paris Sciences et Lettres (PSL) University\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "cv/index.html",
    "href": "cv/index.html",
    "title": "Emory Richardson",
    "section": "",
    "text": "Download current CV"
  },
  {
    "objectID": "publications/index.html",
    "href": "publications/index.html",
    "title": "Publications",
    "section": "",
    "text": "Richardson, E., Davis, M., Isaac, & Keil, F. (2025). Agenda setting and the emperor’s new clothes: People diagnose information cascades during sequential testimony by reasoning about informants’ speaking order and social status. Open Mind. https://doi.org/10.31234/osf.io/vkgz5 ( Manuscript |  code & data)\nRichardson, E., Hok, H., Shaw, A., & Keil, F. (2025). Herding cats: Children and adults infer collective decision speed from team size and diversity, but disagree about whether consensus strength matters more than team size. Cognition, 263, 106211. https://doi.org/10.1016/j.cognition.2025.106211 ( PDF |  code & data)\nMenendez, D., Richardson, E., McNeil, K., & Gelman, S. (2024). Discovering the world of viruses: Testing the influence of anthropomorphic representations on children’s learning about COVID-19. Developmental Psychology, 61, 513–529. https://doi.org/10.1037/dev0001882 ( PDF |  code & data)\nRichardson, E., & Keil, F. (2022). The potential for effective reasoning guides children’s preference for small group discussion over crowdsourcing. Scientific Reports, 12, 1193. https://doi.org/10.1038/s41598-021-04680-z ( PDF |  code & data)\nRichardson, E., & Keil, F. (2022). Thinking takes time: Children use agents’ response times to infer the source, quality, and complexity of their knowledge. Cognition, 224. https://doi.org/10.1016/j.cognition.2022.105073 ( PDF |  code & data)\nRichardson, E., Sheskin, M., & Keil, F. (2021). An illusion of self‐sufficiency for learning about artifacts in scaffolded learners, but not observers. Child Development, 92, 1523–1538. https://doi.org/https://doi.org/10.1111/cdev.13506 ( PDF |  code & data)"
  },
  {
    "objectID": "publications/index.html#peer-reviewed-publications",
    "href": "publications/index.html#peer-reviewed-publications",
    "title": "Publications",
    "section": "",
    "text": "Richardson, E., Davis, M., Isaac, & Keil, F. (2025). Agenda setting and the emperor’s new clothes: People diagnose information cascades during sequential testimony by reasoning about informants’ speaking order and social status. Open Mind. https://doi.org/10.31234/osf.io/vkgz5 ( Manuscript |  code & data)\nRichardson, E., Hok, H., Shaw, A., & Keil, F. (2025). Herding cats: Children and adults infer collective decision speed from team size and diversity, but disagree about whether consensus strength matters more than team size. Cognition, 263, 106211. https://doi.org/10.1016/j.cognition.2025.106211 ( PDF |  code & data)\nMenendez, D., Richardson, E., McNeil, K., & Gelman, S. (2024). Discovering the world of viruses: Testing the influence of anthropomorphic representations on children’s learning about COVID-19. Developmental Psychology, 61, 513–529. https://doi.org/10.1037/dev0001882 ( PDF |  code & data)\nRichardson, E., & Keil, F. (2022). The potential for effective reasoning guides children’s preference for small group discussion over crowdsourcing. Scientific Reports, 12, 1193. https://doi.org/10.1038/s41598-021-04680-z ( PDF |  code & data)\nRichardson, E., & Keil, F. (2022). Thinking takes time: Children use agents’ response times to infer the source, quality, and complexity of their knowledge. Cognition, 224. https://doi.org/10.1016/j.cognition.2022.105073 ( PDF |  code & data)\nRichardson, E., Sheskin, M., & Keil, F. (2021). An illusion of self‐sufficiency for learning about artifacts in scaffolded learners, but not observers. Child Development, 92, 1523–1538. https://doi.org/https://doi.org/10.1111/cdev.13506 ( PDF |  code & data)"
  },
  {
    "objectID": "publications/index.html#conference-proceedings",
    "href": "publications/index.html#conference-proceedings",
    "title": "Publications",
    "section": "Conference Proceedings",
    "text": "Conference Proceedings\n\nRichardson, E., Hok, H., Shaw, A., & Keil, F. (2023). Herding cats: Children’s intuitive theories of persuasion predict slower collective decisions in larger and more diverse groups, but disregard factional power. Proceedings of the Cognitive Science Society, 7. https://escholarship.org/content/qt7sj3b4bv/qt7sj3b4bv.pdf( PDF | SPP Poster\nRichardson, E., Davis, I., & Keil, F. C. (2023). Agenda setting and The Emperor’s New Clothes: People infer that letting powerful agents make their opinion known early can trigger information cascades and pluralistic ignorance. Proceedings of the Cognitive Science Society, 7. https://escholarship.org/content/qt31k4r1vh/qt31k4r1vh.pdf( PDF\nRichardson, E., & Keil, F. C. (2022). “He only changed his answer because they shouted at him”: Children use affective cues to distinguish between genuine and forced consensus. Proceedings of the Cognitive Science Society, 7. https://escholarship.org/uc/item/5m79x64g( PDF | cogsci poster\nRichardson, E., Miro-Rivera, D., & Keil, F. C. (2022). Know your network: People infer cultural drift from network structure, and expect collaborating with more distant experts to improve innovation, but collaborating with network-neighbors to improve memory. Proceedings of the Cognitive Science Society, 7. https://escholarship.org/uc/item/85c8h8wp( PDF | cogsci poster\nRichardson, E., & Keil, F. (2021, June). You can’t trust an angry group- asymmetric evaluations of angry and surprised rhetoric affect confidence in trending opinions. Proceedings of the Cognitive Science Society. https://doi.org/10.31234/osf.io/y3wv2( PDF | cogsci poster\nRichardson, E., & Keil, F. C. (2020). Does informational independence always matter? Children believe small group discussion is more accurate than ten times as many independent informants. Proceedings of the Cognitive Science Society, 7. https://escholarship.org/uc/item/2q69p85v( PDF\nRichardson, E., & Keil, F. C. (2020). Children use agents’ response time to distinguish between memory and novel inference. Proceedings of the Cognitive Science Society, 7. https://escholarship.org/uc/item/5zn4353d( PDF\nRichardson, E., & Jara-Ettinger, J. (2019). You must know something I don’t: Risky behavior implies privileged information. Proceedings of the Cognitive Science Society, 6. https://escholarship.org/uc/item/3r2767kn( PDF | cogsci poster"
  },
  {
    "objectID": "publications/index.html#talks",
    "href": "publications/index.html#talks",
    "title": "Publications",
    "section": "Talks",
    "text": "Talks\n\nPfänder, J., & Mercier, H. (2025, March). Trust in science. Paris School of International Affairs (Sciences Po), Paris.\nPfänder, J., & Altay, S. (2025, March). Spotting False News and Doubting True News. A Meta-Analysis of News Judgments. Infox sur seine, Paris.\nPfänder, J. (2024, July). How wise is the crowd: Can we infer people are accurate and competent merely because they agree with each other? 46th Annual Meeting of the Cognitive Science Society, Rotterdam.\nPfänder, J. (2024, June). Processus sous-tendant la confiance dans la science. 5ème édition des Rencontres Jeunes Chercheur·euse·s, Grenoble.\nPfänder, J., & Altay, S. (2024, February). Spotting False News and Doubting True News: A Meta-Analysis of News Judgments. EDMO Scientific Conference on Disinformation, Amsterdam."
  }
]